<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Principal component analysis</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Analysis</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About Me</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Course
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="pca.html">Principal component analysis</a>
    </li>
    <li>
      <a href="da.html">Linear Discriminant Analysis</a>
    </li>
    <li>
      <a href="fa.html">Factor Analysis</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Principal component analysis</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#what">What are Principal Components?</a></li>
<li><a href="#preparing">Preparing Our Data</a></li>
<li><a href="#selecting">Selecting the Number of Principal Components</a></li>
</ul>
</div>

<p>Principal Component Analysis (PCA) involves the process by which principal components are computed, and their role in understanding the data. PCA is an unsupervised approach, which means that it is performed on a set of variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, …, <span class="math inline">\(X_p\)</span> with no associated response <span class="math inline">\(Y\)</span>. PCA reduces the dimensionality of the data set, allowing most of the variability to be explained using fewer variables. PCA is commonly used as one step in a series of analyses. You can use PCA to reduce the number of variables and avoid multicollinearity, or when you have too many predictors relative to the number of observations.</p>
<div id="what" class="section level2">
<h2>What are Principal Components?</h2>
<p>The goal of PCA is to explain most of the variability in the data with a smaller number of variables than the original data set. For a large data set with <span class="math inline">\(p\)</span> variables, we could examine pairwise plots of each variable against every other variable, but even for moderate <span class="math inline">\(p\)</span>, the number of these plots becomes excessive and not useful. For example, when <span class="math inline">\(p = 10\)</span> there are <span class="math inline">\(p(p-1)/2 = 45\)</span> scatterplots that could be analyzed! Clearly, a better method is required to visualize the n observations when p is large. In particular, we would like to find a low-dimensional representation of the data that captures as much of the information as possible. For instance, if we can obtain a two-dimensional representation of the data that captures most of the information, then we can plot the observations in this low-dimensional space.</p>
<p>PCA provides a tool to do just this. It finds a low-dimensional representation of a data set that contains as much of the variation as possible. The idea is that each of the n observations lives in p-dimensional space, but not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. Each of the dimensions found by PCA is a linear combination of the p features and we can take these linear combinations of the measurements and reduce the number of plots necessary for visual analysis while retaining most of the information present in the data.</p>
<p>We now explain the manner in which these dimensions, or principal components, are found.</p>
<p>The first principal component of a data set <span class="math inline">\(X_1, X_2, ..., X_p\)</span> is the linear combination of the features</p>
<p><span class="math display">\[Z_{1} = \phi_{11}X_{1} + \phi_{21}X_{2} + ... + \phi_{p1}X_{p} \tag{1}\]</span></p>
<p>that has the largest variance and where <span class="math inline">\(\phi_1\)</span> is the first principal component loading vector, with elements <span class="math inline">\(\phi_{12}, \phi_{22}, \dots, \phi_{p2}\)</span>. The <span class="math inline">\(\phi\)</span> are normalized, which means that <span class="math inline">\(\sum_{j=1}^{p}{\phi_{j1}^{2}} = 1\)</span>. After the first principal component <span class="math inline">\(Z_1\)</span> of the features has been determined, we can find the second principal component <span class="math inline">\(Z_2\)</span>. The second principal component is the linear combination of <span class="math inline">\(X_1,\dots , X_p\)</span> that has maximal variance out of all linear combinations that are uncorrelated with <span class="math inline">\(Z_1\)</span>. The second principal component scores <span class="math display">\[z_{12}, z_{22}, \dots, z_{n2}\]</span> take the form</p>
<p><span class="math display">\[Z_{2} = \phi_{12}X_{1} + \phi_{22}X_{2} + ... + \phi_{p2}X_{p} \tag{2}\]</span></p>
<p>This proceeds until all principal components are computed. The elements <span class="math inline">\(\phi_{11}, ..., \phi_{p1}\)</span> in Eq. 1 are the loadings of the first principal component. To calculate these loadings, we must find the <span class="math inline">\(\phi\)</span> vector that maximizes the variance. It can be shown using techniques from linear algebra that the eigenvector corresponding to the largest eigenvalue of the covariance matrix is the set of loadings that explains the greatest proportion of the variability.</p>
<p>Therefore, to calculate principal components, we start by using the <code>cov()</code> function to calculate the covariance matrix, followed by the <code>eigen</code> command to calculate the eigenvalues of the matrix. <code>eigen</code> produces an object that contains both the ordered eigenvalues (<code>values</code>) and the corresponding eigenvector matrix (<code>vectors</code>).</p>
</div>
<div id="preparing" class="section level2">
<h2>Preparing Our Data</h2>
<p>It is usually beneficial for each variable to be centered at zero for PCA, due to the fact that it makes comparing each principal component to the mean straightforward. This also eliminates potential problems with the scale of each variable.</p>
<pre class="r"><code>data=read.table(&quot;http://hamrita.e-monsite.com/medias/files/datatd1.txt&quot;,h=T)
# centering data
data_c=apply(data, 2, scale)</code></pre>
<p>Then, we calculate the eigen values and eigen vectors of the covariance matrix of scaled data</p>
<pre class="r"><code>data=read.table(&quot;http://hamrita.e-monsite.com/medias/files/datatd1.txt&quot;,h=T)
# calculate the covariance matrix
cov_mat=cov(data_c)
cov_mat</code></pre>
<pre><code>           PAO        PAA        VIO        VIA        POT        LEC        RAI
PAO  1.0000000 -0.7736643  0.9261878 -0.9057929  0.6563525  0.8885594 -0.8334273
PAA -0.7736643  1.0000000 -0.6040133  0.9044415 -0.3328879 -0.6733710  0.9588178
VIO  0.9261878 -0.6040133  1.0000000 -0.7501607  0.5170754  0.7917256 -0.6690062
VIA -0.9057929  0.9044415 -0.7501607  1.0000000 -0.4185661 -0.8386021  0.9239285
POT  0.6563525 -0.3328879  0.5170754 -0.4185661  1.0000000  0.6029189 -0.4099317
LEC  0.8885594 -0.6733710  0.7917256 -0.8386021  0.6029189  1.0000000 -0.8244519
RAI -0.8334273  0.9588178 -0.6690062  0.9239285 -0.4099317 -0.8244519  1.0000000
PLP -0.8558457  0.7712209 -0.8279862  0.7197908 -0.5539561 -0.7509231  0.8344475
           PLP
PAO -0.8558457
PAA  0.7712209
VIO -0.8279862
VIA  0.7197908
POT -0.5539561
LEC -0.7509231
RAI  0.8344475
PLP  1.0000000</code></pre>
<pre class="r"><code># eigen values and vectors
eig=eigen(cov_mat)
# eigen values
eig$values</code></pre>
<pre><code>[1]  6.207947e+00  8.796814e-01  4.159611e-01  3.064547e-01  1.684415e-01
[6]  1.806771e-02  3.446769e-03 -7.882727e-17</code></pre>
<pre class="r"><code># eigen vectors
eig$vectors</code></pre>
<pre><code>           [,1]        [,2]        [,3]        [,4]       [,5]        [,6]
[1,]  0.3913106 -0.13782295  0.16171408  0.11934962 -0.2940454  0.39774771
[2,] -0.3486743 -0.44058521  0.31994973  0.21790900  0.2654417  0.52070420
[3,]  0.3491929 -0.20168213  0.68063176 -0.02888337 -0.2457165 -0.46475199
[4,] -0.3736251 -0.26030922  0.07348235 -0.39654491  0.3456048 -0.42286619
[5,]  0.2463714 -0.74382646 -0.55765980 -0.07399153 -0.1757252 -0.10774663
[6,]  0.3648220 -0.12802133  0.03240104  0.51888931  0.6691919 -0.18494157
[7,] -0.3730515 -0.32597990  0.25424981  0.06370633 -0.2715323  0.01626466
[8,] -0.3616760  0.05022725 -0.16169177  0.70810300 -0.3329142 -0.36024492
            [,7]        [,8]
[1,] -0.10692002  0.72896315
[2,]  0.42307943 -0.11777262
[3,]  0.25392333 -0.18012990
[4,]  0.03334541  0.57500037
[5,]  0.09342846 -0.13544943
[6,] -0.31310737  0.01273521
[7,] -0.76590350 -0.15895192
[8,]  0.22496593  0.21885081</code></pre>
<p>For our example, we’ll take the first two sets of loadings and store them in the matrix <code>phi</code></p>
<pre class="r"><code>(phi=eig$vectors[,1:2])</code></pre>
<pre><code>           [,1]        [,2]
[1,]  0.3913106 -0.13782295
[2,] -0.3486743 -0.44058521
[3,]  0.3491929 -0.20168213
[4,] -0.3736251 -0.26030922
[5,]  0.2463714 -0.74382646
[6,]  0.3648220 -0.12802133
[7,] -0.3730515 -0.32597990
[8,] -0.3616760  0.05022725</code></pre>
<p>Eigenvectors that are calculated in any software package are unique up to a sign flip. By default, eigenvectors in R point in the negative direction. For this example, we’d prefer the eigenvectors point in the positive direction because it leads to more logical interpretation of graphical results as we’ll see shortly. To use the positive-pointing vector, we multiply the default loadings by <span class="math inline">\(-1\)</span>. The set of loadings for the first principal component (PC1) and second principal component (PC2) are shown below:</p>
<pre class="r"><code>phi=-phi
colnames(phi)=c(&quot;PC1&quot;, &quot;PC2&quot;)
phi</code></pre>
<pre><code>            PC1         PC2
[1,] -0.3913106  0.13782295
[2,]  0.3486743  0.44058521
[3,] -0.3491929  0.20168213
[4,]  0.3736251  0.26030922
[5,] -0.2463714  0.74382646
[6,] -0.3648220  0.12802133
[7,]  0.3730515  0.32597990
[8,]  0.3616760 -0.05022725</code></pre>
<p>Each principal component vector defines a direction in feature space. Because eigenvectors are orthogonal to every other eigenvector, loadings and, therefore, principal components are uncorrelated with one another, and form a basis of the new space. This holds true no matter how many dimensions are being used.</p>
<p>If we project the n data points <span class="math inline">\(x_1, ..., x_n\)</span> onto the first eigenvector, the projected values are called the principal component scores for each observation.</p>
<pre class="r"><code># Calculate Principal Components scores
PC1 &lt;- as.matrix(data_c) %*% phi[,1]
PC2 &lt;- as.matrix(data_c) %*% phi[,2]

# Create data frame with Principal Components scores
PC &lt;- data.frame(grad = row.names(data), PC1, PC2)
PC</code></pre>
<pre><code>  grad        PC1         PC2
1 AGRI -3.1538232 -0.22993989
2 SAAG -3.2942597 -0.41850307
3 PRIN  1.3769588  0.05473497
4 CSUP  4.0772714  0.16473285
5 CMOY  1.6071145 -0.80132031
6 EMPL  0.7544441 -0.75630756
7 OUVR -0.8410311 -0.17121742
8 INAC -0.5266749  2.15782043</code></pre>
<p>Now that we’ve calculated the first and second principal components.</p>
<pre class="r"><code># Plot Principal Components for each State
library(ggplot2)
ggplot(PC, aes(PC1, PC2)) + 
  modelr::geom_ref_line(h = 0) +
  modelr::geom_ref_line(v = 0) +
  geom_text(aes(label = grad), size = 3) +
  xlab(&quot;First Principal Component&quot;) + 
  ylab(&quot;Second Principal Component&quot;) + 
  ggtitle(&quot;First Two Principal Components of Data&quot;)</code></pre>
<p><img src="pca_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Because PCA is unsupervised, this analysis on its own is not making predictions, but simply making connections between observations using fewer measurements.</p>
</div>
<div id="selecting" class="section level2">
<h2>Selecting the Number of Principal Components</h2>
<p>Note that in the above analysis we only looked at two of the four principal components. How did we know to use two principal components? And how well is the data explained by these two principal components compared to using the full data set?</p>
<div id="the-proportion-of-variance-explained" class="section level3">
<h3>The Proportion of Variance Explained</h3>
<p>We mentioned previously that PCA reduces the dimensionality while explaining most of the variability, but there is a more technical method for measuring exactly what percentage of the variance was retained in these principal components.</p>
<p>By performing some algebra, the proportion of variance explained (PVE) by the mth principal component is calculated using the equation:</p>
<p><span class="math display">\[PVE = \frac{\sum^n_{i=1}(\sum^p_{j=1}\phi_{jm}x_{ij})^2}{\sum^p_{j=1}\sum^n_{i=1}x^2_{ij}} \tag{3}\]</span></p>
<p>It can be shown that the PVE of the mth principal component can be more simply calculated by taking the mth eigenvalue and dividing it by the number of principal components, p. A vector of PVE for each principal component is calculated:</p>
<pre class="r"><code>PVE &lt;- eig$values / sum(eig$values)
round(PVE, 2)</code></pre>
<pre><code>[1] 0.78 0.11 0.05 0.04 0.02 0.00 0.00 0.00</code></pre>
<p>The first principal component in our example therefore explains 78% of the variability, and the second principal component explains 11%. Together, the first two principal components explain 89% of the variability.</p>
<p>It is often advantageous to plot the PVE and cumulative PVE, for reasons explained in the following section of this tutorial. The plot of each is shown below:</p>
<pre class="r"><code># PVE (aka scree) plot
library(ggplot2); library(gridExtra)
PVEplot &lt;- qplot(c(1:8), PVE) + 
  geom_line() + 
  xlab(&quot;Principal Component&quot;) + 
  ylab(&quot;PVE&quot;) +
  ggtitle(&quot;Scree Plot&quot;) +
  ylim(0, 1)

# Cumulative PVE plot
cumPVE &lt;- qplot(c(1:8), cumsum(PVE)) + 
  geom_line() + 
  xlab(&quot;Principal Component&quot;) + 
  ylab(NULL) + 
  ggtitle(&quot;Cumulative Scree Plot&quot;) +
  ylim(0,1)

grid.arrange(PVEplot, cumPVE, ncol = 2)</code></pre>
<pre><code>Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<pre><code>Warning: Removed 1 row(s) containing missing values (geom_path).</code></pre>
<p><img src="pca_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
